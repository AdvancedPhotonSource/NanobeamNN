{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e70b6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from math import *\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from mpl_toolkits.axes_grid1.axes_divider import make_axes_locatable\n",
    "from matplotlib.colors import Normalize\n",
    "import cmcrameri.cm as cmc\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a83eca",
   "metadata": {},
   "source": [
    "## Load simulation data + pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc2fca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '/YOUR/DATA/SAVING/PATH'\n",
    "\n",
    "sim_mat = np.load(os.path.join(data_folder, 'sim_mat.npy'))\n",
    "\n",
    "# This order assumes dim. 0: strain, 1: tilt_lr, 2: tilt_ud, as generated using the simulation.py script\n",
    "sim_mat = np.reshape(sim_mat, (sim_mat.shape[0]*sim_mat.shape[1]*sim_mat.shape[2], \n",
    "                               sim_mat.shape[3], sim_mat.shape[4]))\n",
    "\n",
    "strain = np.linspace(-0.005, 0.005, 41) # Strain range\n",
    "tilt_lr = np.linspace(-0.05, 0.05, 41) # Tilt_LR range in degrees\n",
    "tilt_ud = np.linspace(-0.1, 0.1, 41) # Tilt_UD range in degrees\n",
    "\n",
    "labels = np.zeros((41, 41, 41, 3))\n",
    "for p0 in range(labels.shape[0]):\n",
    "    for p1 in range(labels.shape[1]):\n",
    "        for p2 in range(labels.shape[2]):\n",
    "            labels[p0, p1, p2] = np.array([strain[p0], tilt_lr[p1], tilt_ud[p2]])\n",
    "labels = np.reshape(labels, (labels.shape[0]*labels.shape[1]*labels.shape[2], labels.shape[3]))\n",
    "labels[:, 0] *= 100 # Weight the physical parameters equally, same order of magnitude\n",
    "labels[:, 1] *= 10 \n",
    "labels[:, 2] *= 5\n",
    "\n",
    "print('Data shape: ', sim_mat.shape, '| Labels shape: ', labels.shape)\n",
    "            \n",
    "labels = np.float32(np.around(labels, 5))\n",
    "sim_mat = (sim_mat / np.max(sim_mat)) * 7 # Normalize the intensity to approximate experimental values\n",
    "\n",
    "# Add noise to emulate diffraction experiment\n",
    "rng = np.random.default_rng()\n",
    "for i in tqdm(range(sim_mat.shape[0])):\n",
    "    sim_mat[i] = rng.poisson(sim_mat[i])\n",
    "    \n",
    "# Round the data to the nearest integer (number of photons) and convert to float32\n",
    "sim_mat = np.rint(sim_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1311e9f",
   "metadata": {},
   "source": [
    "## Prepare data for PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d091e14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make pytorch Dataset\n",
    "\n",
    "class SimDataset(Dataset):\n",
    "    \"\"\"Simulated diffraction dataset. Labels for params: strain (0), tilt_lr (1), tilt_ud (2), in order.\"\"\"\n",
    "    \n",
    "    def __init__(self, data, params, transform=None):\n",
    "        \"\"\"\n",
    "        data (numpy array): simulated diffraction patterns\n",
    "        params (numpy array): labels\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.params = params\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        image = self.data[idx]\n",
    "        lattice = self.params[idx]\n",
    "        sample = {'image': image, 'lattice': lattice}\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        return sample\n",
    "    \n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert numpy arrays in samples to Tensors\"\"\"\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        image = sample['image']\n",
    "        lattice = sample['lattice']\n",
    "        return {'image': torch.unsqueeze(torch.from_numpy(image), 0), 'lattice': torch.from_numpy(lattice)}\n",
    "    \n",
    "# Initialize\n",
    "diff_dataset = SimDataset(data=sim_mat, params=labels, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e35964",
   "metadata": {},
   "source": [
    "## Hyperparameters and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b58e577",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NGPUS = torch.cuda.device_count() # Uses all available GPUs\n",
    "NGPUS = 1\n",
    "BATCH_SIZE = NGPUS * 64\n",
    "LR = 0.0001 * NGPUS\n",
    "print(\"GPUs:\", NGPUS, \"| Batch size:\", BATCH_SIZE, \"| Learning rate:\", LR)\n",
    "\n",
    "EPOCHS = 500\n",
    "MODEL_SAVE_PATH = '/MODEL/SAVE/PATH'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e25327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training, validation, and test sets\n",
    "\n",
    "generator0 = torch.Generator().manual_seed(8)\n",
    "subsets = torch.utils.data.random_split(diff_dataset, [0.8, 0.1, 0.1], generator=generator0)\n",
    "\n",
    "# Use a DataLoader to iterate through the Datasets\n",
    "\n",
    "trainloader = DataLoader(subsets[0], batch_size=BATCH_SIZE, shuffle=True)\n",
    "validloader = DataLoader(subsets[1], batch_size=BATCH_SIZE, shuffle=True)\n",
    "testloader = DataLoader(subsets[2], batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9fefa8",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ec9cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional Neural Network\n",
    "\n",
    "class NanobeamNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NanobeamNN, self).__init__()\n",
    "        \n",
    "        self.operation = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=2, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(2, 4, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 2)),\n",
    "            \n",
    "            nn.Conv2d(4, 8, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 16, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 2)),\n",
    "            \n",
    "            nn.Conv2d(16, 32, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 2))\n",
    "        )\n",
    "        self.fc1 = nn.Linear(1024, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.operation(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ef93c7",
   "metadata": {},
   "source": [
    "## Check that all dimensions are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be82ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = NanobeamNN()\n",
    "for i, data in enumerate(trainloader):\n",
    "    inputs, labels = data['image'], data['lattice']\n",
    "    print(\"inputs:\", inputs.shape, labels.shape)\n",
    "    outputs = cnn(inputs)\n",
    "    print(\"outputs:\", outputs.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9d3ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(cnn, (1, 1, 64, 64), device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a08aafe",
   "metadata": {},
   "source": [
    "## Move model to device "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcdf29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = NanobeamNN()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if NGPUS > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "    cnn = nn.DataParallel(cnn) #Default all devices\n",
    "\n",
    "cnn = cnn.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5849c602",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00536bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(cnn.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b67f593",
   "metadata": {},
   "source": [
    "# Training and Validation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c6344a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update saved model if validation loss is minimum\n",
    "def update_saved_model(model, path, name):\n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)\n",
    "    for f in os.listdir(path):\n",
    "        os.remove(os.path.join(path, f))\n",
    "    torch.save(model.module.state_dict(), os.path.join(path, name))\n",
    "    \n",
    "def generate_state_dict(epoch_num, metrics, optimizer):\n",
    "    \"\"\"\n",
    "    Returns a dictionary of the state_dicts of all states but not the model.\n",
    "    \"\"\"\n",
    "    state = {\n",
    "        'current_epoch': epoch_num + 1,\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss_tracker': metrics\n",
    "    }\n",
    "    return state\n",
    "\n",
    "def save_model_and_states_checkpoint(model, path, epoch_num, metrics, optimizer):\n",
    "    \"\"\"Save a checkpoint state that can be loaded to continue training.\"\"\"\n",
    "    state_dict = generate_state_dict(epoch_num, metrics, optimizer)\n",
    "    torch.save(state_dict, os.path.join(path, 'checkpoint.state'))\n",
    "    update_saved_model(model, path, 'checkpoint_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0125c52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(trainloader, metrics):\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, data in enumerate(trainloader):\n",
    "        inputs, labels = data['image'].to(device), data['lattice'].to(device)\n",
    "        \n",
    "        outputs = cnn(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.detach().item()\n",
    "        \n",
    "    metrics['losses'].append(running_loss/i)\n",
    "    \n",
    "def validate(validloader, metrics):\n",
    "    tot_val_loss = 0.0\n",
    "    \n",
    "    for j, sample in enumerate(validloader):\n",
    "        images, ground_truth = sample['image'].to(device), sample['lattice'].to(device)\n",
    "        \n",
    "        predicted = cnn(images)\n",
    "        \n",
    "        val_loss = criterion(predicted, ground_truth)\n",
    "        tot_val_loss += val_loss.detach().item()\n",
    "        \n",
    "    metrics['val_losses'].append(tot_val_loss/j)\n",
    "        \n",
    "    if (tot_val_loss/j < metrics['best_val_loss']):\n",
    "        print(\"Saving improved model after Val. Loss improved from %.5f to %.5f\" \n",
    "              % (metrics['best_val_loss'], tot_val_loss/j))\n",
    "        metrics['best_val_loss'] = tot_val_loss/j\n",
    "        update_saved_model(cnn, MODEL_SAVE_PATH, 'best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c36f7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics = {'losses': [], 'val_losses': [], 'best_val_loss': np.inf}\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    \n",
    "    #Set model to train mode\n",
    "    cnn.train()\n",
    "    \n",
    "    #Training loop\n",
    "    train(trainloader, metrics)\n",
    "    \n",
    "    #Switch model to eval mode\n",
    "    cnn.eval()\n",
    "    \n",
    "    #Validation loop\n",
    "    validate(validloader, metrics)\n",
    "    \n",
    "    print('Epoch: %d | Train Loss: %.5f | Val. Loss: %.5f'\n",
    "          %(epoch, metrics['losses'][-1], metrics['val_losses'][-1]))\n",
    "    \n",
    "save_model_and_states_checkpoint(cnn, MODEL_SAVE_PATH, epoch, metrics, optimizer)\n",
    "    \n",
    "print('Finished Training')\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9987e19b",
   "metadata": {},
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dece42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Functions for organizing the model statistics\n",
    "\n",
    "DEVICE = \"cpu\"\n",
    "\n",
    "def get_predictions(model, dataloader, batch_size=BATCH_SIZE, device=DEVICE):\n",
    "    \"\"\"Returns network predictions and labels by any NanobeamNN model on any SimDataset.\"\"\"\n",
    "    \n",
    "    pred_vals = np.zeros((len(dataloader)-1, batch_size, 3))\n",
    "    gt_vals = np.zeros(pred_vals.shape)\n",
    "    model.to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloader):\n",
    "            images = data['image'].to(device)\n",
    "            labels = data['lattice'].to(device)\n",
    "            outputs = model(images)\n",
    "            if i < pred_vals.shape[0]:\n",
    "                pred_vals[i] = outputs.detach().cpu().numpy()\n",
    "                gt_vals[i] = labels.detach().cpu().numpy()\n",
    "            else:\n",
    "                # The last batch may be a different size\n",
    "                pred_vals_last = outputs.detach().cpu().numpy()\n",
    "                gt_vals_last = labels.detach().cpu().numpy()\n",
    "    \n",
    "    pred_vals = np.reshape(pred_vals, (pred_vals.shape[0]*pred_vals.shape[1], pred_vals.shape[2]))\n",
    "    gt_vals = np.reshape(gt_vals, (gt_vals.shape[0]*gt_vals.shape[1], gt_vals.shape[2]))\n",
    "    pred_vals = np.vstack((pred_vals, pred_vals_last))\n",
    "    gt_vals = np.vstack((gt_vals, gt_vals_last))\n",
    "    \n",
    "    return pred_vals, gt_vals\n",
    "\n",
    "def make_idx_dict(keys, label_arr, param_num):\n",
    "    \"\"\"Returns a dictionary with labels as keys and numpy arrays of indices where the label is the key.\n",
    "    keys: iterable list-like structure of label values\n",
    "    label_arr: numpy array of labels\n",
    "    param_num: 0 = strain, 1 = tilt_lr, 2 = tilt_ud\n",
    "    \"\"\"\n",
    "    idx_dict = dict.fromkeys(keys)\n",
    "    for key in keys:\n",
    "        val_arr = np.argwhere(np.around(label_arr[:, param_num], 3) == np.around(key, 3))\n",
    "        idx_dict[key] = val_arr\n",
    "    return idx_dict\n",
    "\n",
    "def test_make_idx_dict(idx_dict):\n",
    "    \"\"\"Return the total number of indices accounted for. It should be equal to the total number of labels.\"\"\"\n",
    "    counter = 0\n",
    "    for key in idx_dict.keys():\n",
    "        counter += idx_dict[key].shape[0]\n",
    "    return counter\n",
    "\n",
    "def get_pred_err_stats(idx_arr, pred_arr, param_num):\n",
    "    \"\"\"Returns the mean and standard deviation of the prediction error for a parameter.\n",
    "    idx_arr: array of indices\n",
    "    pred_arr: array of predictions on a set of data\n",
    "    param_num: 0 = strain, 1 = thickness, 2 = tilt_lr, 3 = tilt_ud\n",
    "    \"\"\"\n",
    "    temp = np.zeros((idx_arr.shape[0], ))\n",
    "    j = 0\n",
    "    for idx in idx_arr:\n",
    "        temp[j] = pred_arr[idx[0], param_num]\n",
    "        j += 1\n",
    "    return np.mean(temp), np.std(temp)   \n",
    "\n",
    "def combine_stats(idx_dict, pred_arr, param_num):\n",
    "    stats_arr = np.zeros((len(idx_dict), 2))\n",
    "    counter = 0\n",
    "    for key in idx_dict.keys():\n",
    "        i_arr = idx_dict[key]\n",
    "        avg, stddev = get_pred_err_stats(i_arr, pred_arr, param_num)\n",
    "        stats_arr[counter, 0] = avg\n",
    "        stats_arr[counter, 1] = stddev\n",
    "        counter += 1\n",
    "    print(counter)\n",
    "    return stats_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02033c0d",
   "metadata": {},
   "source": [
    "## Load trained model and make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3112f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = NanobeamNN()\n",
    "cnn.load_state_dict(torch.load('trained_model.pth'))\n",
    "\n",
    "pred_train, train_labels = get_predictions(cnn, trainloader)\n",
    "pred_test, test_labels = get_predictions(cnn, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dc2832",
   "metadata": {},
   "outputs": [],
   "source": [
    "strain_label_range = np.linspace(-0.005, 0.005, 41)*100\n",
    "print('Strain label range (not actual value range): ', strain_label_range)\n",
    "tilt_lr_label_range = np.linspace(-0.05, 0.05, 41)*10\n",
    "print('Tilt_lr label range: ', tilt_lr_label_range)\n",
    "tilt_ud_label_range = np.linspace(-0.1, 0.1, 41)*5\n",
    "print('Tilt_ud label range: ', tilt_ud_label_range)\n",
    "\n",
    "# Make dictionaries for training and test labels\n",
    "train_s_dict = make_idx_dict(strain_label_range, train_labels, 0)\n",
    "train_lr_dict = make_idx_dict(tilt_lr_label_range, train_labels, 1)\n",
    "train_ud_dict = make_idx_dict(tilt_ud_label_range, train_labels, 2)\n",
    "print('Total indices (train):', test_make_idx_dict(train_s_dict), test_make_idx_dict(train_lr_dict),\n",
    "      test_make_idx_dict(train_ud_dict))\n",
    "\n",
    "test_s_dict = make_idx_dict(strain_label_range, test_labels, 0)\n",
    "test_lr_dict = make_idx_dict(tilt_lr_label_range, test_labels, 1)\n",
    "test_ud_dict = make_idx_dict(tilt_ud_label_range, test_labels, 2)\n",
    "print('Total indices (test):', test_make_idx_dict(test_s_dict), test_make_idx_dict(test_lr_dict), \n",
    "      test_make_idx_dict(test_ud_dict))\n",
    "\n",
    "# Compile the prediction statistics\n",
    "train_stats_s = combine_stats(train_s_dict, pred_train, 0)\n",
    "train_stats_lr = combine_stats(train_lr_dict, pred_train, 1)\n",
    "train_stats_ud = combine_stats(train_ud_dict, pred_train, 2)\n",
    "\n",
    "test_stats_s = combine_stats(test_s_dict, pred_test, 0)\n",
    "test_stats_lr = combine_stats(test_lr_dict, pred_test, 1)\n",
    "test_stats_ud = combine_stats(test_ud_dict, pred_test, 2)\n",
    "\n",
    "# Distribution of absolute prediction error\n",
    "x0 = train_labels[:, 0]/100 - pred_train[:, 0]/100\n",
    "y0 = test_labels[:, 0]/100 - pred_test[:, 0]/100\n",
    "range0 = np.max([np.abs(x0.min()), np.abs(x0.max()), np.abs(y0.min()), np.abs(y0.max())])\n",
    "bins0 = np.linspace(-range0, range0, 20)\n",
    "\n",
    "x1 = train_labels[:, 1]/10 - pred_train[:, 1]/10\n",
    "y1 = test_labels[:, 1]/10 - pred_test[:, 1]/10\n",
    "range1 = np.max([np.abs(x1.min()), np.abs(x1.max()), np.abs(y1.min()), np.abs(y1.max())])\n",
    "bins1 = np.linspace(-range1, range1, 20)\n",
    "\n",
    "x2 = train_labels[:, 2]/10 - pred_train[:, 2]/10\n",
    "y2 = test_labels[:, 2]/10 - pred_test[:, 2]/10\n",
    "range2 = np.max([np.abs(x2.min()), np.abs(x2.max()), np.abs(y2.min()), np.abs(y2.max())])\n",
    "bins2 = np.linspace(-range2, range2, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758d4261",
   "metadata": {},
   "source": [
    "## Prediction statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77897773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of predicted values by parameter \n",
    "# Refer to Figure S2a in the Supplementary Information\n",
    "\n",
    "f, ax = plt.subplots(figsize=(15, 4), ncols=3)\n",
    "\n",
    "ax[0].errorbar(np.linspace(-0.005, 0.005, 41), train_stats_s[:, 0]/100, yerr=train_stats_s[:, 1]/100, \n",
    "               label='train', fmt='none', capsize=3, color='blue')\n",
    "ax[0].errorbar(np.linspace(-0.005, 0.005, 41)+0.0001, test_stats_s[:, 0]/100, yerr=test_stats_s[:, 1]/100, \n",
    "               label='test', fmt='none', capsize=3, color='red')\n",
    "ax[0].set_xlabel('Ground truth')\n",
    "ax[0].set_ylabel('Predicted')\n",
    "ax[0].set_title('Strain')\n",
    "ax[0].set_aspect(1)\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].errorbar(np.linspace(-0.05, 0.05, 41), train_stats_lr[:, 0]/10, yerr=train_stats_lr[:, 1]/10, \n",
    "               label='train', fmt='none', capsize=3, color='blue')\n",
    "ax[1].errorbar(np.linspace(-0.05, 0.05, 41)+0.0015, test_stats_lr[:, 0]/10, yerr=test_stats_lr[:, 1]/10, \n",
    "               label='test', fmt='none', capsize=3, color='red')\n",
    "ax[1].set_xlabel('Ground truth')\n",
    "ax[1].set_ylabel('Predicted')\n",
    "ax[1].set_title('In-plane rotation (deg.)')\n",
    "ax[1].set_aspect(1)\n",
    "\n",
    "ax[2].errorbar(np.linspace(-0.1, 0.1, 41), train_stats_ud[:, 0]/5, yerr=train_stats_ud[:, 1]/5, \n",
    "               label='train', fmt='none', capsize=3, color='blue')\n",
    "ax[2].errorbar(np.linspace(-0.1, 0.1, 41)+0.0025, test_stats_ud[:, 0]/5, yerr=test_stats_ud[:, 1]/5, \n",
    "               label='test', fmt='none', capsize=3, color='red')\n",
    "ax[2].set_xlabel('Ground truth')\n",
    "ax[2].set_ylabel('Predicted')\n",
    "ax[2].set_title('Out-of-plane rotation (deg.)')\n",
    "ax[2].set_aspect(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7c1228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of absolute error by parameter\n",
    "# Refer to Figure S2b in the Supplementary Information\n",
    "\n",
    "f, ax = plt.subplots(figsize=(14, 4), ncols=3)\n",
    "\n",
    "ax[0].hist([x0, y0], bins0, label=['train', 'test'])\n",
    "ax[0].set_xlabel('Pred. error')\n",
    "ax[0].set_title('Strain')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].hist([x1, y1], bins1, label=['train', 'test'])\n",
    "ax[1].set_xlabel('Pred. error (deg.)')\n",
    "ax[1].set_title('In-plane rotation')\n",
    "\n",
    "ax[2].hist([x2, y2], bins2, label=['train', 'test'])\n",
    "ax[2].set_xlabel('Pred. error (deg.)')\n",
    "ax[2].set_title('Out-of-plane rotation')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18f5105",
   "metadata": {},
   "source": [
    "## Diffraction comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3101fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate simulated diffraction patterns from NanobeamNN predicted parameters\n",
    "upsampling1 = 1\n",
    "\n",
    "energy = 11.3\n",
    "wavelength = 12.398/energy\n",
    "K=2*pi/wavelength\n",
    "c = 4.013\n",
    "l = 2\n",
    "alf0 = asin(wavelength*l/2/c)\n",
    "alf = alf0 \n",
    "twotheta = (2 * alf0) \n",
    "X0 =  256 \n",
    "Xcen = 256 \n",
    "\n",
    "distance = 0.85\n",
    "pixelsize = 55e-6/upsampling1*2 # The *2 is for binning\n",
    "\n",
    "gam0 = twotheta-alf\n",
    "\n",
    "focal_length = 21.874e-3\n",
    "outer_angle = 149e-6/2/focal_length # diameter of FZP is 150 um\n",
    "inner_angle = 77e-6/2/focal_length # diameter of CS is 75 um \n",
    "\n",
    "precision = 5e-4 # for fast numerical integration\n",
    "\n",
    "det_x = np.arange(64*upsampling1).astype(np.float64)\n",
    "det_y = np.arange(64*upsampling1).astype(np.float64)\n",
    "det_x = det_x - det_x.mean() + X0 - (X0-Xcen)*upsampling1\n",
    "det_y -= det_y.mean()\n",
    "\n",
    "det_xx, det_yy = np.meshgrid(det_x,det_y)\n",
    "\n",
    "gam = np.arcsin((det_xx-X0)*pixelsize/distance)+gam0\n",
    "\n",
    "#detector\n",
    "det_Qx = K*(np.cos(alf)-np.cos(gam))\n",
    "det_Qz = K*(np.sin(gam)+np.sin(alf))\n",
    "det_Qy = det_yy*pixelsize/distance*K\n",
    "\n",
    "upsampling2 = 2\n",
    "\n",
    "O_x = np.arange(64*upsampling2).astype(np.float64)\n",
    "O_y = np.arange(64*upsampling2).astype(np.float64)\n",
    "O_x -= O_x.mean()\n",
    "O_y -= O_y.mean()\n",
    "\n",
    "O_xx, O_yy = np.meshgrid(O_x,O_y)\n",
    "O_xx = O_xx[:,:,np.newaxis,np.newaxis]\n",
    "O_yy = O_yy[:,:,np.newaxis,np.newaxis]\n",
    "\n",
    "# origin of the reciprocal space\n",
    "O_Qx = -O_xx*pixelsize*upsampling1/upsampling2/distance*K*sin(alf)\n",
    "O_Qz = O_xx*pixelsize*upsampling1/upsampling2/distance*K*cos(alf) # the sign of Qx and Qz are opposite in this convention\n",
    "O_Qy = O_yy*pixelsize*upsampling1/upsampling2/distance*K\n",
    "O_angle = np.sqrt(O_yy**2+O_xx**2)*pixelsize*upsampling1/upsampling2/distance\n",
    "O_donut = (O_angle < outer_angle) * (O_angle > inner_angle)\n",
    "\n",
    "def Thickness_Fringe(thickness=117, strain=0, tilt_lr=0, tilt_ud=0, precision=5e-4):\n",
    "\n",
    "    return ((thickness*np.sinc(thickness*(det_Qz-2*pi/c*l/(1+strain)-O_Qz)/pi/2)**2 *\\\n",
    "            (np.abs(det_Qx+2*pi/c*l/(1+strain)*radians(tilt_lr)-O_Qx)<precision) *\\\n",
    "            (np.abs(det_Qy+2*pi/c*l/(1+strain)*radians(tilt_ud)-O_Qy)<precision))*O_donut)\\\n",
    "            .sum(axis=(0,1)).reshape(64,upsampling1,64,upsampling1).mean(axis=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e8760c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose some points from the test data set to compare input images and simulation(output values)\n",
    "idx_list = np.array([183, 1046, 5021, 5706]) # Random, test set is not shuffled for reproducibility\n",
    "\n",
    "plot_images = np.zeros((4, 64, 64)) # Input images\n",
    "plot_labels = np.zeros((4, 3)) # Ground truth labels\n",
    "for i in range(len(idx_list)):\n",
    "    for j, data in enumerate(testloader):\n",
    "        if idx_list[i] // batch_size == j:\n",
    "            images, labels = data['image'], data['lattice']\n",
    "            plot_images[i] = images[idx_list[i] % batch_size].squeeze().detach().numpy()\n",
    "            plot_labels[i] = labels[idx_list[i] % batch_size].squeeze().detach().numpy()\n",
    "            \n",
    "sim_pred = np.zeros((4, 64, 64))\n",
    "for i in range(sim_pred.shape[0]):\n",
    "    sim_pred[i] = Thickness_Fringe(117, pred_test[idx_list[i], 0]/100, \n",
    "                                   pred_test[idx_list[i], 1]/10, pred_test[idx_list[i], 2]/5)\n",
    "\n",
    "sim_pred = (sim_pred / np.max(sim_pred)) * 7\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "for i in range(sim_pred.shape[0]):\n",
    "    sim_pred[i] = rng.poisson(sim_pred[i])\n",
    "\n",
    "# Round the data to the nearest integer, but keep dtype as float32\n",
    "sim_pred = np.rint(sim_pred).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52782713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the comparison between input images and simulated patterns from output parameters\n",
    "vmin = 1\n",
    "vmax = 7\n",
    "normalizer = colors.LogNorm(vmin, vmax)\n",
    "im = cm.ScalarMappable(norm=normalizer, cmap='jet')\n",
    "\n",
    "f, ax = plt.subplots(figsize=(11, 4.5), nrows=2, ncols=4)\n",
    "\n",
    "ax[0, 0].imshow(plot_images[0], interpolation='none', cmap='jet', norm=normalizer)\n",
    "ax[0, 0].set_xticks([])\n",
    "ax[0, 0].set_yticks([])\n",
    "ax[0, 0].set_title('Input 0')\n",
    "\n",
    "ax[0, 1].imshow(plot_images[1], interpolation='none', cmap='jet', norm=normalizer)\n",
    "ax[0, 1].set_xticks([])\n",
    "ax[0, 1].set_yticks([])\n",
    "ax[0, 1].set_title('Input 1')\n",
    "\n",
    "ax[0, 2].imshow(plot_images[2], interpolation='none', cmap='jet', norm=normalizer)\n",
    "ax[0, 2].set_xticks([])\n",
    "ax[0, 2].set_yticks([])\n",
    "ax[0, 2].set_title('Input 2')\n",
    "\n",
    "ax[0, 3].imshow(plot_images[3], interpolation='none', cmap='jet', norm=normalizer)\n",
    "ax[0, 3].set_xticks([])\n",
    "ax[0, 3].set_yticks([])\n",
    "ax[0, 3].set_title('Input 3')\n",
    "\n",
    "ax[1, 0].imshow(sim_pred[0], interpolation='none', cmap='jet', norm=normalizer)\n",
    "ax[1, 0].set_xticks([])\n",
    "ax[1, 0].set_yticks([])\n",
    "ax[1, 0].set_title('Sim. from pred. 0')\n",
    "\n",
    "ax[1, 1].imshow(sim_pred[1], interpolation='none', cmap='jet', norm=normalizer)\n",
    "ax[1, 1].set_xticks([])\n",
    "ax[1, 1].set_yticks([])\n",
    "ax[1, 1].set_title('Sim. from pred. 1')\n",
    "\n",
    "ax[1, 2].imshow(sim_pred[2], interpolation='none', cmap='jet', norm=normalizer)\n",
    "ax[1, 2].set_xticks([])\n",
    "ax[1, 2].set_yticks([])\n",
    "ax[1, 2].set_title('Sim. from pred. 2')\n",
    "\n",
    "ax[1, 3].imshow(sim_pred[3], interpolation='none', cmap='jet', norm=normalizer)\n",
    "ax[1, 3].set_xticks([])\n",
    "ax[1, 3].set_yticks([])\n",
    "ax[1, 3].set_title('Sim. from pred. 3')\n",
    "\n",
    "f.subplots_adjust(hspace=0.15, wspace=0.05, right=0.9)\n",
    "cbar_ax = f.add_axes([0.90, 0.12, 0.02, 0.75])\n",
    "f.colorbar(im, cax=cbar_ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0473eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selected points from cell above, refer to Figure 2a of the main article\n",
    "\n",
    "vmin = 1\n",
    "vmax = 8\n",
    "normalizer = colors.LogNorm(vmin, vmax)\n",
    "im = cm.ScalarMappable(norm=normalizer, cmap=cmc.lapaz)\n",
    "\n",
    "f, ax = plt.subplots(figsize=(3.5, 3), nrows=2, ncols=2, dpi=300)\n",
    "\n",
    "ax[0, 0].imshow(plot_images[0], interpolation='none', norm=normalizer, cmap=cmc.lapaz)\n",
    "ax[0, 0].set_xticks([])\n",
    "ax[0, 0].set_yticks([])\n",
    "ax[0, 0].set_ylabel('Simulated')\n",
    "\n",
    "ax[0, 1].imshow(plot_images[2], interpolation='none', norm=normalizer, cmap=cmc.lapaz)\n",
    "ax[0, 1].set_xticks([])\n",
    "ax[0, 1].set_yticks([])\n",
    "\n",
    "ax[1, 0].imshow(sim_pred[0], interpolation='none', norm=normalizer, cmap=cmc.lapaz)\n",
    "ax[1, 0].set_xticks([])\n",
    "ax[1, 0].set_yticks([])\n",
    "ax[1, 0].set_ylabel('Predicted')\n",
    "\n",
    "ax[1, 1].imshow(sim_pred[2], interpolation='none', norm=normalizer, cmap=cmc.lapaz)\n",
    "ax[1, 1].set_xticks([])\n",
    "ax[1, 1].set_yticks([])\n",
    "\n",
    "f.subplots_adjust(hspace=0.15, wspace=0, right=0.85)\n",
    "cbar_ax = f.add_axes([0.85, 0.12, 0.03, 0.75])\n",
    "f.colorbar(im, cax=cbar_ax, label='Photons')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
